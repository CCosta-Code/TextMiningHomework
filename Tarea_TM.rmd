---
title: "Tarea Text Mining"
author: "Rodrigo Zúñiga - Celeste Romero - Javier Ortiz - Carolina Costa"
date: "25-10-2019"
output:
  prettydoc::html_pretty:
    theme: lumen
    highlight: github
---

#### **Corrección de errores principales de codificación de caracteres**

##### Primero se marcan las letras especiales simbolos ascii ej: Ñ -> #Ñ$. Luego se convierten de utf-8 a ascii y de vuelta a UTF-8, para eliminar caracteres. Finalemnte, se reconvierten los caracteres especiales ej: #Ñ$ -> Ñ.
```{r}

corregir_textos <- function(string){
  
  #####
  # PRIMERA PARTE: Se homologan vocales con tilde, sin tilde y la letra Ñ.
  #####
  
  Encoding(string) <- "UTF-8"
  
  string <- stringr::str_replace_all(string, "Ã\u0091|Ñ", "#N$")
  string <- stringr::str_replace_all(string, "Ã±|ñ", "#n$")
  
  string <- stringr::str_replace_all(string, "Ã\u0081|Á", "#A$")
  string <- stringr::str_replace_all(string, "Ã\u0089|É", "#E$")
  string <- stringr::str_replace_all(string, "Ã\u008d|Í", "#I$")
  string <- stringr::str_replace_all(string, "Ã\u0093|Ó", "#O$")
  string <- stringr::str_replace_all(string, "Ã\u009a|Ú", "#U$")
  
  string <- stringr::str_replace_all(string, "Ã¡|á", "#a$")
  string <- stringr::str_replace_all(string, "Ã©|é", "#e$")
  string <- stringr::str_replace_all(string, "Ã³|ó", "#o$")
  string <- stringr::str_replace_all(string, "Ãº|ú", "#u$")
  
# OBS: La letra i con tilde queda al final porque el caracter que lo acompaña no se logró filtrar aparte.
  string <- stringr::str_replace_all(string, "Ã|í", "#i$")
  
  # Se cambia el Non-breaking space por un espacio normal
  string <- stringr::str_replace_all(string, "\u00A0", " ")
  
  
  #####
  # SEGUNDA PARTE: Se eliminan caracteres no ascii
  #####
  string <- iconv(string, "utf-8", "ascii", sub="")
  string <- iconv(string, "ascii", "utf-8", sub="")

  #####
  # TERCERA PARTE: Se regeneran las vocales con tilde y la Ñ
  #####
  string <- stringr::str_replace_all(string, "#N\\$", "Ñ")
  string <- stringr::str_replace_all(string, "#n\\$", "ñ")

  string <- stringr::str_replace_all(string, "#A\\$", "Á")
  string <- stringr::str_replace_all(string, "#E\\$", "É")
  string <- stringr::str_replace_all(string, "#I\\$", "Í")
  string <- stringr::str_replace_all(string, "#O\\$", "Ó")
  string <- stringr::str_replace_all(string, "#U\\$", "Ú")

  string <- stringr::str_replace_all(string, "#a\\$", "á")
  string <- stringr::str_replace_all(string, "#e\\$", "é")
  string <- stringr::str_replace_all(string, "#i\\$", "í")
  string <- stringr::str_replace_all(string, "#o\\$", "ó")
  string <- stringr::str_replace_all(string, "#u\\$", "ú")
  
  # OBS: Si hay varios espacios se colapsan en uno solo
  string <- stringr::str_replace_all(string, "\\s+", " ")
  # OBS: Se eliminan datos basura encontrados en 13 registros
  string <- stringr::str_replace_all(string, "\\(function \\(\\) \\{.+\\}\\)\\(\\);", "")
  
  return(string)
}

# OBS: Con la función anterior se reemplazan las letras con tilde a las mismas pero sin el tilde
elimina_tildes <- function(string){
  Encoding(string) <- "UTF-8"
  string <- stringr::str_replace_all(string, "Á", "A")
  string <- stringr::str_replace_all(string, "É", "E")
  string <- stringr::str_replace_all(string, "Í", "I")
  string <- stringr::str_replace_all(string, "Ó", "O")
  string <- stringr::str_replace_all(string, "Ú", "U")
  
  string <- stringr::str_replace_all(string, "á", "a")
  string <- stringr::str_replace_all(string, "é", "e")
  string <- stringr::str_replace_all(string, "í", "i")
  string <- stringr::str_replace_all(string, "ó", "o")
  string <- stringr::str_replace_all(string, "ú", "u")
  
  return(string)
}

```



#### **Lectura y manipulación de datos**

##### En este apartado se levantan los datos del csv, se renombran variables convenientemente, y se aplican las funciones presentadas anteriormente para limpiar el texto. Finalmente, se crea la función para generar la *Matriz de datos* 

```{r}

cargar_csv_as_tibble <- function(file = 'inputs_csv/df_EmpleoPublico.csv'){
  
  datos <- readr::read_csv(file)

  d <- datos %>%
    ## Crear una variable de identidicación
    tibble::rownames_to_column("doc_id") %>%

    ## Renombrar columnas
    rename(Institucion_Entidad = `InstituciÃ³n / Entidad`) %>%
    rename(Numero_de_vacantes = `NÂº de Vacantes`) %>%
    rename(Objetivo_del_cargo = `Objetivo del Cargo`) %>%
    rename(Region = `RegiÃ³n`) %>%
    rename(Tipo_de_vacante = `Tipo de Vacante`) %>%
    rename(Area_de_trabajo = `Ãrea de Trabajo`) %>%
    
    ## Se limpian los contenidos de las columnas con texto
    mutate(doc_id = as.integer(doc_id)) %>%
    mutate(Cargo = corregir_textos(Cargo)) %>%
    mutate(Ciudad = corregir_textos(Ciudad)) %>%
    mutate(Institucion_Entidad = corregir_textos(Institucion_Entidad)) %>%
    mutate(Ministerio = corregir_textos(Ministerio)) %>%
    mutate(Objetivo_del_cargo = corregir_textos(Objetivo_del_cargo)) %>%
    mutate(Region = corregir_textos(Region)) %>%
    mutate(Tipo_de_vacante = corregir_textos(Tipo_de_vacante)) %>%
    mutate(Area_de_trabajo = corregir_textos(Area_de_trabajo)) %>%
    
    ## Eliminar columnas con datos vacíos
    select_if(~sum(!is.na(.)) > 0)
  
  return(d)
}


# OBS: Nueva función para levantar los csv de "Meses" y "Ciudades/Comunas"
cargar_csv_as_bow <- function(file){
  datos <- readr::read_csv(file)
  
  d <- datos %>%
    select(Nombre) %>%
    mutate(Nombre = tolower(Nombre)) %>%
    mutate(Nombre2 = elimina_tildes(Nombre)) %>%
    tidyr::gather() %>%
    select(value) %>%
    distinct() %>% #View()
    as.vector() %>%
    tm::VectorSource() %>%
    tm::Corpus() %>%
    tm::tm_map(tm::removeNumbers) %>%
    tm::tm_map(tolower) %>%
    tm::tm_map(tm::removePunctuation) %>%
    tm::tm_map(function(x) tm::removeWords(x, tm::stopwords("es"))) %>%
    tm::TermDocumentMatrix() %>%
    tm::weightTf() %>%
    extract2(6) %>%
    extract2(1)
  
  return(d)
}


CrearMatriz <- function(corpus, type='TermDocument', weight='Tf'){
  # Se utiliza la funcion "tm_map" de tm para limpiar/normalizar palabras del corpus
  
  # OBS: Se crea una función que reemplaza un patron dado por un espacio. 
  # Se usa un espacio y no simplemente eliminar, para que no se junten palabras de textos mal escritos. Ej: Supervisor (a)Asesor (a) Unidades Educativas
  (to_spaces <- tm::content_transformer(function(x, pattern) gsub(pattern, " ", x)))
  
  # Se utiliza la función to_spaces para eliminar lo que parezca una URL
  corpus <- tm::tm_map(corpus, to_spaces, "(http[^ ]*)|(ftp[^ ]*)|(www\\.[^ ]*)")
  
  # Eliminar números, convertir a minúscula, remover espacios, y stop words
  corpus <- tm::tm_map(corpus, tm::removeNumbers)
  corpus <- tm::tm_map(corpus, tolower)
  corpus <- tm::tm_map(corpus, to_spaces, "[[:punct:]]")
  #corpus <- tm::tm_map(corpus, tm::removePunctuation)
  corpus <- tm::tm_map(corpus, function(x) tm::removeWords(x, tm::stopwords("es")))
  
  # Levantar datos de meses
  meses <- cargar_csv_as_bow('inputs_csv/meses.csv')
  corpus <- tm::tm_map(corpus, function(x) tm::removeWords(x, meses))
  
  # Levantar datos de comunas y ciudades del país
  lugares <- cargar_csv_as_bow('inputs_csv/regiones-provincias-comunas.csv')
  corpus <- tm::tm_map(corpus, function(x) tm::removeWords(x, lugares))
  
  # Quedarse con la raíz de cada palabra
  corpus <- tm::tm_map(corpus, tm::stemDocument, language = "spanish")
  
  # Crear matrix de terminos desde el corpus
  if(type == 'TermDocument'){
    dtm <- tm::TermDocumentMatrix(corpus)
  }else if (type == 'DocumentTerm') {
    dtm <- tm::DocumentTermMatrix(corpus)
  } else {
    return(1)
  }
  
  # Generar la matrix de pesos TF (o tambien puede usar: TF x IDF)
  if(weight == 'Tf'){
    dtm <- tm::weightTf(dtm)
  }else if (weight == 'TfIdf') {
    dtm <- tm::weightTfIdf(dtm)
  } else {
    return(2)
  }
  
  return(dtm)
}
```




#### **Creación del CORPUS**

##### A continuación se seleccionan las variables a incluir en el corpus, se crea una variable común con esa información concatenada, y se eliminan los documentos sin datos

```{r}

library(dplyr)
library(magrittr)

# OSB: Los datos de empleo se guardaton en un rds para faciliar su lectura
datos_empleos <- readRDS('df_EmpleosPublicos.RDS')


df_corpus <- datos_empleos %>%
  # Se seleccionan sólo las variables de interés para incorporar en el corpus
  select(doc_id, Cargo, Objetivo_del_cargo, Area_de_trabajo) %>%
  
  # Remover aquellos documentos con datos vacíos
  filter(!rowSums(is.na(.)) >= 3) %>%
  
  # Crear una variable que contenga la información de las tras variables de interés a incluir en el corpus
  mutate(text = paste(if_else(is.na(Cargo), "", Cargo), 
                      if_else(is.na(Objetivo_del_cargo), "", Objetivo_del_cargo),
                      if_else(is.na(Area_de_trabajo), "", Area_de_trabajo),
                      sep = " "
                      )
        ) %>%
  select(doc_id, text) %>%
  as.data.frame()

# Creación del corpus
corpus <- tm::Corpus(tm::DataframeSource(df_corpus))

#tm::inspect(corpus[30149])
#tm::inspect(tm::tm_map(corpus[30149], tm::removePunctuation))
#tm::inspect(corpus[1:2])
#NLP::meta(corpus[[2]], "text")

# Correr la función CrearMatriz, se utiliza la matriz de pesos de frecuencias Tf y se mostrará como resultado una matriz de Documentos (filas) por Términos (columnas)
dtm <- CrearMatriz(corpus, type='DocumentTerm', weight='Tf')

#dtm[,30149]
#tm::inspect(dtm[,30149])
#tm::inspect(corpus[38611])
#View(dtm$dimnames$Terms)

```



#### **Modelo Latent Dirichlet Allocation (LDA)**

##### En esta sección se aplicará la metodología de LDA para realizar una división de tópicos que sirvan para mejorar las búsquedas de empleos

```{r}

burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

# Se testearon distintos números de tópicos, la prueba con 100 tópicos ha mejorado los resultados de las búsquedas. Se presume que con mayor número de tópicos esta podría mejorar aun más, pero las capacidades computacionales no permiten realizar esas pruebas
NumTopicos <- 100

# Generar el modelo de topicos usando LDA con muestreo basado en método de Gibbs (se registrará el tiempo de corrida para estimar el tiempo en caso de incremento en el número de tópicos)

## OBS: Esta parte será comentada y a continuación se leerá el archivo ldaOut100.RDS 
#t <- proc.time()
#MatrizDatos <- as.matrix(dtm)
#ldaOut <-topicmodels::LDA(dtm,k=NumTopicos,method ="Gibbs",control=list(nstart=nstart, #seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
#proc.time() - t

#saveRDS(ldaOut, 'ldaOut100.RDS')

```




#### **Funciones de búsqueda**

##### _**buscar_topico()**_: a partir de un modelo LDA y un string de busqueda dado, procesa el string tal como se hizo en el corpus original para obtener las palabras que debieran buscarse en el modelo (filtradas y con stemming). Luego se obtiene la probabilidad de cada tópico, para cada uno de los terminos buscados. Se suman las probabilidades de cada termino por cada uno de los tópicos, obteniendo una lista de tópicos para la busqueda completa. Finalmente se retorna el tópico con el valor más alto.

```{r}

buscar_topico <- function(ldaModel, texto = 'ingeniero eléctrico'){
  # Se construye un microcorpus de 1 solo documento y se obtiene la matriz document term
  doc_id <- c(0)
  text <- texto
  consulta <- data.frame(doc_id, text )
  corpus_consulta <- tm::Corpus(tm::DataframeSource(consulta))
  # Se desactivan los warnings de la funcion crear matriz y luego los activamos nuevamente
  oldw <- getOption("warn")
  options(warn = -1)
  dtm_consulta <- CrearMatriz(corpus_consulta, type='DocumentTerm', weight='Tf')
  options(warn = oldw)
  
  # Se extraen los terminos finales y se guardan en un vector ej: ingeniero -> ingenier
  terminos <- dtm_consulta$dimnames$Terms
  
  # De la matriz beta, que tiene la probabilidad de cada topico, para cada termino, se filtran los terminos que se están buscando.
  # A continuación se agrupan por topico, se suma la probabilidad de cada palabra, y se ordenan los topicos de mayor a menor.
  topics_probabilities <- tidytext::tidy(ldaModel, matrix = "beta") %>%
    filter(term %in% terminos) %>%
    group_by(topic) %>%
    summarise(total = sum(beta)) %>%
    arrange(desc(total))
  
  # Se solicita mostrar el mas alto
  top <- topics_probabilities %>% select(topic) %>%head(1) %>% as.integer()
  
  return(top)
}

```



##### _**documentos_por_topico()**_: Dado el modelo y el tópico, se retorna los IDs de los 10 documentos principales asociados a dicho tópico. Finalmente, se utiliza la función tm::inspect() para filtrar el corpus a partir de los id obtenidos. 

```{r}

documentos_por_topico <- function(ldaModel, topico = 1){
  documentos_id <- tidytext::tidy(ldaModel, matrix = "gamma") %>%
    filter(topic == topico) %>%
    arrange(desc(gamma)) %>%
    head(10) %>%
    pull(document)
  
  return(documentos_id)
}

```


#### **Ejemplo de búsqueda de empleos**

```{r}

# Levantar los datos:
ldaOut_guardado <- readRDS('ldaOut100.RDS')

# Búsqueda con buenos resultados
topico <- buscar_topico(ldaOut_guardado, 'computación')
documentos <- documentos_por_topico(ldaOut_guardado, topico)
tm::inspect(corpus_guardado[documentos])

# Búsqueda con malos resultados
topico <- buscar_topico(ldaOut_guardado, 'ingeniero electrónico')
documentos <- documentos_por_topico(ldaOut_guardado, topico)
tm::inspect(corpus_guardado[documentos])

# Búsqueda con buenos resultados
topico <- buscar_topico(ldaOut_guardado, 'abogado')
documentos <- documentos_por_topico(ldaOut_guardado, topico)
tm::inspect(corpus_guardado[documentos])

# Búsqueda con buenos resultados
topico <- buscar_topico(ldaOut_guardado, 'educadora de párvulos')
documentos <- documentos_por_topico(ldaOut_guardado, topico)
tm::inspect(corpus_guardado[documentos])

# Búsqueda con buenos resultados (salen cosas de conaf)
topico <- buscar_topico(ldaOut_guardado, 'agrónomo')
documentos <- documentos_por_topico(ldaOut_guardado, topico)
tm::inspect(corpus_guardado[documentos])

# Búsqueda con malos resultados
topico <- buscar_topico(ldaOut_guardado, 'sociólogo')
documentos <- documentos_por_topico(ldaOut_guardado, topico)
tm::inspect(corpus_guardado[documentos])
```


Observaciones:

#### _* Como se describió en el desarrollo del trabajo, distintos números de tópicos fueron probados. Valores menores a 50 no generaban los resultados deseados. El correr 100 tópicos significó más de 9 horas de procesamiento, este número de tópicos mejoró los resultados obtenidos, pero el nivel esperado aún no es el deseado por el equipo. Se hace la suposición que aumentar un poco más el número de tópicos (por ejemplo a 150 - 170), podría mejorar un poco más los resultados al realizar las búsquedas debido a la amplia variedad de cargos ofrecidos por las diferentes instituciones_

#### _* Trabajando con este número de tópicos, y el total de documentos disponibles en el corpus las búsquedas de prueba funcionan medianamente bien en su mayoría. Pero en una profesión como **"ingeniero"** los empleos propuestos por las funciones de búsqueda creadas no arrojan buenos resultados. Estimamos que esto puede deberse a la variedad de profesiones que pueden acompañarla. El equipo ha discutido una posible solución frente a estos casos, la misma refiere a la utilización de un modelo Latent Semantic Analysis (LSA). La principal razón de esta propuesta es que al utilizar esta técnica se gana eficiencia computacional al reducir las dimensiones, y además se elimina la variabilidad dentro de la data, aquel "ruido" que está generando polisemia y también sinonomia en nuestras búsquedas con las funciones y matriz lda aquí propuestas_